# generated by datamodel-codegen:
#   filename:  parallel-execution.json
#   timestamp: 2026-01-25T13:38:25+00:00

from __future__ import annotations

from enum import StrEnum
from typing import Annotated, Any

from pydantic import BaseModel, ConfigDict, Field


class Flow(BaseModel):
    """
    Flow to execute (without environment/globalVariables)
    """

    model_config = ConfigDict(
        populate_by_name=True,
    )
    id: str
    name: str
    actions: list[dict[str, Any]]
    edges: list[dict[str, Any]]


class Mode(StrEnum):
    """
    Execution mode
    """

    full = 'full'
    partial = 'partial'


class FlowExecutionRequest(BaseModel):
    """
    Request to execute a single test/flow
    """

    model_config = ConfigDict(
        populate_by_name=True,
    )
    flow: Any
    """
    Flow to execute (without environment/globalVariables)
    """
    mode: Mode = Mode.full
    """
    Execution mode
    """
    recording: dict[str, Any] | None = None
    """
    Recording configuration
    """
    metadata: dict[str, Any] = {}
    """
    Additional test metadata
    """


class Status(StrEnum):
    """
    Execution status
    """

    completed = 'completed'
    error = 'error'
    stopped = 'stopped'
    running = 'running'
    pending = 'pending'


class FlowExecutionResult(BaseModel):
    """
    Result of a single test execution
    """

    model_config = ConfigDict(
        populate_by_name=True,
    )
    flow_id: str
    """
    ID of the executed flow
    """
    flow_name: str
    """
    Name of the executed flow
    """
    success: bool
    """
    Whether the test passed
    """
    status: Status
    """
    Execution status
    """
    total_actions: Annotated[int, Field(ge=0)]
    """
    Total actions in the flow
    """
    executed_actions: Annotated[int, Field(ge=0)]
    """
    Number of actions executed
    """
    successful_actions: Annotated[int, Field(ge=0)]
    """
    Number of successful actions
    """
    duration_seconds: Annotated[float, Field(ge=0.0)]
    """
    Total execution time in seconds
    """
    report: dict[str, Any]
    """
    Full flow report
    """
    error: str | None = None
    """
    Error message if failed
    """
    metadata: dict[str, Any] = {}
    """
    Additional test metadata
    """


class BrowserMode(StrEnum):
    """
    Browser mode
    """

    headed = 'headed'
    headless = 'headless'


class ParallelTestsRequest(BaseModel):
    """
    Request to execute multiple tests in parallel
    """

    model_config = ConfigDict(
        populate_by_name=True,
    )
    tests: list[FlowExecutionRequest]
    """
    List of tests to execute
    """
    data: dict[str, Any]
    """
    Shared data containing environment and globalVariables for all tests
    """
    max_parallel: Annotated[int, Field(ge=1, le=50)] = 10
    """
    Maximum number of parallel executions
    """
    stop_on_failure: bool = False
    """
    Stop all tests if one fails
    """
    cleanup_after: bool = True
    """
    Cleanup resources after execution
    """
    browser_mode: BrowserMode = BrowserMode.headed
    """
    Browser mode
    """
    incognito: bool = False
    """
    Whether to use incognito/private mode
    """
    browser: str = 'chrome'
    """
    Browser type (chrome, firefox, etc.)
    """
    max_retries: Annotated[int, Field(ge=0, le=10)] = 3
    """
    Maximum retries for transient failures
    """


class ParallelTestsResult(BaseModel):
    """
    Result of parallel test execution
    """

    model_config = ConfigDict(
        populate_by_name=True,
    )
    total_tests: Annotated[int, Field(ge=0)]
    """
    Total number of tests
    """
    passed_tests: Annotated[int, Field(ge=0)]
    """
    Number of passed tests
    """
    failed_tests: Annotated[int, Field(ge=0)]
    """
    Number of failed tests
    """
    skipped_tests: Annotated[int | None, Field(ge=0)] = 0
    """
    Number of skipped tests
    """
    total_duration_seconds: Annotated[float, Field(ge=0.0)]
    """
    Total execution time
    """
    results: list[FlowExecutionResult]
    """
    Individual test results
    """
    success: bool
    """
    Overall success status
    """


class ParallelExecution(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True,
    )
    requests: list[ParallelTestsRequest] | None = None
    results: list[ParallelTestsResult] | None = None


class ParallelExecutionModels(BaseModel):
    """
    Schema for parallel test execution requests and results
    """

    model_config = ConfigDict(
        populate_by_name=True,
    )
    parallelExecution: ParallelExecution | None = None
